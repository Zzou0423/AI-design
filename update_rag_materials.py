#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RAG è¯­æ–™æ›´æ–°è„šæœ¬

æ‰«æ rag_materials/ æ–‡ä»¶å¤¹ä¸­çš„PDFæ–‡ä»¶ï¼Œå¹¶æ›´æ–°å‘é‡æ•°æ®åº“
æ”¯æŒå¢é‡æ›´æ–°ï¼šåªå¤„ç†æ–°å¢æˆ–ä¿®æ”¹çš„æ–‡ä»¶
"""

import os
import sys
from pathlib import Path
from datetime import datetime
import json
import hashlib
from dotenv import load_dotenv

# Windowsç»ˆç«¯ç¼–ç ä¿®å¤
if sys.platform == 'win32':
    try:
        sys.stdout.reconfigure(encoding='utf-8')
        sys.stderr.reconfigure(encoding='utf-8')
    except:
        pass

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from app.core.vector_store import SurveyVectorStore

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®æ–‡ä»¶è·¯å¾„
INDEX_FILE = project_root / "rag_materials" / ".rag_index.json"


def get_file_hash(file_path: Path) -> str:
    """è®¡ç®—æ–‡ä»¶MD5å“ˆå¸Œå€¼"""
    hash_md5 = hashlib.md5()
    with open(file_path, "rb") as f:
        for chunk in iter(lambda: f.read(4096), b""):
            hash_md5.update(chunk)
    return hash_md5.hexdigest()


def load_index() -> dict:
    """åŠ è½½æ–‡ä»¶ç´¢å¼•"""
    if INDEX_FILE.exists():
        with open(INDEX_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {}


def save_index(index: dict):
    """ä¿å­˜æ–‡ä»¶ç´¢å¼•"""
    INDEX_FILE.parent.mkdir(parents=True, exist_ok=True)
    
    # æ›´æ–°å¤„ç†æ—¶é—´æˆ³
    index["_last_updated"] = datetime.now().isoformat()
    
    with open(INDEX_FILE, 'w', encoding='utf-8') as f:
        json.dump(index, f, ensure_ascii=False, indent=2)


def scan_pdf_files(materials_dir: Path) -> list:
    """æ‰«ææ–‡ä»¶å¤¹ä¸­çš„PDFæ–‡ä»¶"""
    pdf_files = []
    
    if not materials_dir.exists():
        print(f"[è­¦å‘Š] è¯­æ–™æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {materials_dir}")
        return pdf_files
    
    # æ”¯æŒåµŒå¥—æ–‡ä»¶å¤¹
    for pdf_file in materials_dir.rglob("*.pdf"):
        if pdf_file.is_file():
            pdf_files.append(pdf_file)
    
    return sorted(pdf_files)


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 70)
    print("RAG è¯­æ–™æ›´æ–°å·¥å…·")
    print("=" * 70)
    
    # æ£€æŸ¥API Key
    if not os.getenv("DASHSCOPE_API_KEY"):
        print("\n[é”™è¯¯] DASHSCOPE_API_KEY æœªè®¾ç½®")
        print("è¯·åœ¨ .env æ–‡ä»¶ä¸­é…ç½®æ‚¨çš„ DashScope API Key")
        return 1
    
    # è¯­æ–™æ–‡ä»¶å¤¹è·¯å¾„
    materials_dir = project_root / "rag_materials"
    
    # æ‰«æPDFæ–‡ä»¶
    print(f"\nğŸ“‚ æ‰«æè¯­æ–™æ–‡ä»¶å¤¹: {materials_dir}")
    pdf_files = scan_pdf_files(materials_dir)
    
    if not pdf_files:
        print("\n[è­¦å‘Š] æœªæ‰¾åˆ°PDFæ–‡ä»¶")
        print(f"è¯·å°†é—®å·æ ·ä¾‹PDFæ–‡ä»¶æ”¾å…¥: {materials_dir}")
        print("\næ”¯æŒçš„æ ¼å¼:")
        print("  - PDFæ–‡ä»¶ (.pdf)")
        print("  - æ”¯æŒåµŒå¥—æ–‡ä»¶å¤¹")
        return 0
    
    print(f"[æˆåŠŸ] æ‰¾åˆ° {len(pdf_files)} ä¸ªPDFæ–‡ä»¶:\n")
    for i, pdf_file in enumerate(pdf_files, 1):
        rel_path = pdf_file.relative_to(materials_dir)
        print(f"  {i}. {rel_path}")
    
    # åŠ è½½æ–‡ä»¶ç´¢å¼•
    index = load_index()
    
    # æ£€æŸ¥éœ€è¦å¤„ç†çš„æ–‡ä»¶ï¼ˆæ–°å¢æˆ–ä¿®æ”¹ï¼‰
    files_to_process = []
    for pdf_file in pdf_files:
        rel_path = str(pdf_file.relative_to(materials_dir))
        file_hash = get_file_hash(pdf_file)
        file_stat = pdf_file.stat()
        
        if rel_path not in index:
            # æ–°æ–‡ä»¶
            files_to_process.append((pdf_file, rel_path, "æ–°å¢"))
            index[rel_path] = {
                "hash": file_hash,
                "size": file_stat.st_size,
                "modified": file_stat.st_mtime,
                "processed_at": None
            }
        elif index[rel_path]["hash"] != file_hash:
            # ä¿®æ”¹è¿‡çš„æ–‡ä»¶
            files_to_process.append((pdf_file, rel_path, "æ›´æ–°"))
            index[rel_path]["hash"] = file_hash
            index[rel_path]["size"] = file_stat.st_size
            index[rel_path]["modified"] = file_stat.st_mtime
    
    if not files_to_process:
        print("\n[ä¿¡æ¯] æ‰€æœ‰æ–‡ä»¶éƒ½å·²å¤„ç†ï¼Œæ— éœ€æ›´æ–°")
        return 0
    
    print(f"\n[å¤„ç†] éœ€è¦å¤„ç† {len(files_to_process)} ä¸ªæ–‡ä»¶:\n")
    for pdf_file, rel_path, status in files_to_process:
        print(f"  [{status}] {rel_path}")
    
    # åˆå§‹åŒ–å‘é‡å­˜å‚¨
    print("\n[åˆå§‹åŒ–] åˆå§‹åŒ–å‘é‡æ•°æ®åº“...")
    vector_store = SurveyVectorStore(
        persist_directory="./data/chroma_db",
        collection_name="exemplary_surveys"
    )
    
    # å°è¯•åŠ è½½ç°æœ‰å‘é‡å­˜å‚¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºæ–°çš„
    try:
        vector_store.create_vector_store()
        print("[æˆåŠŸ] å‘é‡æ•°æ®åº“åŠ è½½æˆåŠŸ")
        is_new = False
    except Exception as e:
        print(f"[ä¿¡æ¯] å‘é‡æ•°æ®åº“ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºæ–°çš„æ•°æ®åº“")
        print(f"       é”™è¯¯ä¿¡æ¯: {e}")
        is_new = True
    
    # å¤„ç†æ¯ä¸ªæ–‡ä»¶
    print("\n[å¤„ç†] å¼€å§‹å¤„ç†æ–‡ä»¶...\n")
    processed_count = 0
    failed_files = []
    
    for pdf_file, rel_path, status in files_to_process:
        try:
            print(f"[{status}] å¤„ç†æ–‡ä»¶: {rel_path}")
            
            # åŠ è½½PDFå¹¶åˆ‡åˆ†
            documents = vector_store.load_and_split_pdf(str(pdf_file))
            
            if is_new and processed_count == 0:
                # ç¬¬ä¸€ä¸ªæ–‡ä»¶ï¼Œåˆ›å»ºæ–°çš„å‘é‡å­˜å‚¨
                vector_store.create_vector_store(documents)
                vector_store.persist()
                is_new = False
                print(f"  [æˆåŠŸ] å‘é‡æ•°æ®åº“å·²åˆ›å»ºï¼ŒåŒ…å« {len(documents)} ä¸ªæ–‡æ¡£å—")
            else:
                # åç»­æ–‡ä»¶ï¼Œæ·»åŠ åˆ°ç°æœ‰å‘é‡å­˜å‚¨
                if not vector_store.vector_store:
                    vector_store.create_vector_store()
                vector_store.add_documents(documents)
                vector_store.persist()
                print(f"  [æˆåŠŸ] å·²æ·»åŠ åˆ°å‘é‡æ•°æ®åº“ï¼ŒåŒ…å« {len(documents)} ä¸ªæ–‡æ¡£å—")
            
            # æ›´æ–°ç´¢å¼•
            index[rel_path]["processed_at"] = datetime.now().isoformat()
            processed_count += 1
            
        except Exception as e:
            print(f"  [å¤±è´¥] å¤„ç†å¤±è´¥: {e}")
            failed_files.append((rel_path, str(e)))
    
    # ä¿å­˜ç´¢å¼•
    save_index(index)
    
    # æ˜¾ç¤ºç»“æœ
    print("\n" + "=" * 70)
    print("å¤„ç†å®Œæˆ")
    print("=" * 70)
    print(f"[æˆåŠŸ] æˆåŠŸå¤„ç†: {processed_count}/{len(files_to_process)} ä¸ªæ–‡ä»¶")
    
    if failed_files:
        print(f"\n[å¤±è´¥] å¤±è´¥æ–‡ä»¶ ({len(failed_files)} ä¸ª):")
        for rel_path, error in failed_files:
            print(f"  - {rel_path}: {error}")
    
    # æ˜¾ç¤ºå‘é‡æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯
    if processed_count > 0:
        print("\n[ç»Ÿè®¡] å‘é‡æ•°æ®åº“ç»Ÿè®¡:")
        stats = vector_store.get_stats()
        for key, value in stats.items():
            print(f"  {key}: {value}")
    
    print("\n[æç¤º]")
    print("  - å‘é‡æ•°æ®åº“å·²æ›´æ–°ï¼Œä¸‹æ¬¡ç”Ÿæˆé—®å·æ—¶å°†ä½¿ç”¨æ–°çš„è¯­æ–™")
    print("  - å¦‚éœ€é‡æ–°æ„å»ºå‘é‡æ•°æ®åº“ï¼Œåˆ é™¤ data/chroma_db æ–‡ä»¶å¤¹åé‡æ–°è¿è¡Œæ­¤è„šæœ¬")
    
    return 0 if not failed_files else 1


if __name__ == "__main__":
    try:
        exit_code = main()
        sys.exit(exit_code)
    except KeyboardInterrupt:
        print("\n\n[å–æ¶ˆ] æ“ä½œå·²å–æ¶ˆ")
        sys.exit(1)
    except Exception as e:
        print(f"\n[é”™è¯¯] å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

